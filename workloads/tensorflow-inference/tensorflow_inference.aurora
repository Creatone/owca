# Copyright (c) 2018 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import os

include('../common.aurora')

#----------------------------------------------------------------------------------------------------
###
# Params which can be modified by exporting environment variables.
###

tensorflow_inference_image_tag = os.environ.get('tensorflow_inference_image_tag')  # Workaround for "force pull images" defect in Aurora.

slo = os.environ.get('tf_inference_slo')
#----------------------------------------------------------------------------------------------------


# Add label for identification workload in prometheus.
wrapper_labels["name"] = 'tensorflow_inference--%s' % workload_uniq_id
wrapper_labels["load_generator"] = ""
wrapper_labels["application"] = "tensorflow_inference"

# resources parameters
cpu = 1
ram = 2 * GB
disk = 2 * GB

# JOB definitions
jobs = [
    Service(
        name='tensorflow_inference--%s' % workload_uniq_id,
        cluster=cluster,
        environment='staging' + env_uniq_id,
        role=role,
        enable_hooks=True,
        constraints=dict(own_ip=application_host_ip),
        container=Mesos(image=DockerImage(
            name=docker_registry + '/serenity/tensorflow-inference', tag=tensorflow_inference_image_tag,
        )),
        task=SequentialTask(
            name='tensorflow_inference--%s' % workload_uniq_id,
            resources=Resources(cpu=cpu, ram=ram, disk=disk),
            processes=[
                Process(name='fetch_imagenet_weights', cmdline="ln -s /root/.keras ${HOME}/.keras"),
                Process(
                    name='tensorflow_inference_run',
                    cmdline="KMP_BLOCKTIME=0 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 KMP_SETTINGS=1 OMP_NUM_THREADS={cpus} "
                            "/wrapper.pex --command 'inference --dataset_path '/' --metric_frequency=1' "
                            "--metric_name_prefix 'tensorflow_inference_' "
                            "--stderr 0 --kafka_brokers '{kafka_brokers}' --kafka_topic {kafka_topic} "
                            "--log_level {log_level} "
                            "--slo {slo} --sli_metric_name tensorflow_inference_images_processed_per_second --inverse_sli_metric_value "
                            "--peak_load 1 --load_metric_name const "
                            "--labels \"{labels}\"".format(kafka_brokers=wrapper_kafka_brokers,
                                                           log_level=wrapper_log_level,
                                                           kafka_topic='owca_apms_tensorflow_inference',
                                                           labels=str(wrapper_labels),
                                                           slo=slo,
                                                           cpus=cpu)
                )
            ],
        )
    )
]

hooks = [AddMetadata(wrapper_labels)]
