# Common for all workloads.

# Variables of meaning in accordance with shell command:
# $ aurora job create {cluster}/{role}/staging_{env_uniq_id}/{workload_uniq_id}
cluster = os.environ.get('cluster', 'example')
role = os.environ.get('role', os.environ['USER'])
env_uniq_id = os.environ['env_uniq_id']
workload_uniq_id = os.environ.get('workload_uniq_id')  # can return None

# Note: for workloads like tensorflow ignore load_generator_host_ip.
application_host_ip = os.environ['application_host_ip']
load_generator_host_ip = os.environ['load_generator_host_ip']

docker_registry = os.environ['docker_registry']

# apms -> application performance metrics
apms_kafka_topic_name = os.environ.get('apms_kafka_topic_name', 'owca_apms')

# Wrapper variables:
wrapper_kafka_brokers = os.environ['wrapper_kafka_brokers']
wrapper_prometheus_port = os.environ.get('wrapper_prometheus_port', '9090')
wrapper_prometheus_address= os.environ.get('wrapper_prometheus_address', '0.0.0.0')
wrapper_log_level = os.environ.get('wrapper_log_level', 'DEBUG')
# Here as dict, must be passed to wrapper as json string.
#   Can be extended as desired in workload's aurora manifests.
wrapper_labels = {
    'workload_uniq_id': workload_uniq_id,
    'env_uniq_id': env_uniq_id,
    'own_ip': application_host_ip,
}

#Pre 0.20 way of adding metadata
class AddMetadata:

    def __init__(self, labels):
        self.labels = labels		
    
    def pre_create_job(self, config):
        for label_nama, label_value in self.labels.items():
            config.add_metadata(label_nama, label_value)
        return config
